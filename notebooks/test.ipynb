{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e61c835",
   "metadata": {},
   "source": [
    "# Mean-Field AOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0527583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af405508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from mf_AOA import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18a78c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sherrington-Kirkpatrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3230a6fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# schedule\n",
    "p = 1000\n",
    "τ = 0.5\n",
    "\n",
    "γ = τ * (np.arange(1, p + 1) - 1/2) / p\n",
    "β = τ * (1 - np.arange(1, p + 1) / p)\n",
    "β[p-1] = τ / (4 * p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da610a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# path to folder with the h5 files\n",
    "PATH_DB = \"/home/lappet/Dropbox/MidnightRambler/Mean_Field_AOA/Figures/Data/SK_model/Averaged_MF_energy/\"\n",
    "PATH = \"/home/lappet/Archives/projects/QuantumComputing/QAA_and_MFQAOA/data/mean-field-qaoa/results/sherrington-kirkpatrick/new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948ea0e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed = 137\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0baa8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a47d252",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:00<10:36, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK_model_p_1000_N_9_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:33<00:00, 22.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# for num in [10 + k * 5 for k in range(23)]:\n",
    "for num in [9]:\n",
    "    N = num\n",
    "    num_instances = 10000\n",
    "\n",
    "    file = \"SK_model_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "    print(file)\n",
    "\n",
    "    h5_file = h5py.File(PATH + file, \"w\")\n",
    "\n",
    "    # Sherrington-Kirkpatrick model\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(num_instances)):\n",
    "        J = np.random.normal(0, 1, size=(N, N)) / np.sqrt(N)\n",
    "        J = np.triu(J, k=1)\n",
    "        J = J + J.transpose()\n",
    "\n",
    "        S = np.array([[1., 0., 0.] for _ in range(N - 1)]) # fix final spin (i.e. leave it out)\n",
    "        S = evolve(S, J, β, γ)\n",
    "\n",
    "        h5_file.create_dataset(\"set_\" + str(i + 1) + \"/J\", data=J)\n",
    "        h5_file.create_dataset(\"set_\" + str(i + 1) + \"/S_z\", data=S[:, 2])\n",
    "\n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572b0ab",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Histogram statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3708c64",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK_model_p_1000_N_9_num_inst_10000_seed_137.h5\n"
     ]
    }
   ],
   "source": [
    "seed = 137\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 9\n",
    "num_instances = 10000\n",
    "\n",
    "file = \"SK_model_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "print(file)    \n",
    "data_file = h5py.File(PATH + file, \"r\")\n",
    "\n",
    "E_stars = []\n",
    "for i in range(num_instances):\n",
    "    J = data_file[\"set_\" + str(i + 1) + \"/J\"][:]\n",
    "    S_z = data_file[\"set_\" + str(i + 1) + \"/S_z\"][:]\n",
    "\n",
    "    E_star = expectation(solution(S_z), J)\n",
    "    E_stars.append(E_star)\n",
    "\n",
    "file = \"SK_model_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \"_all_E_stars.h5\"    \n",
    "out_file = h5py.File(PATH_DB + file, \"w\")\n",
    "out_file.create_dataset(\"all_E_stars\", data=E_stars)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055439d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b01e19",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "E_0 = -0.763\n",
    "\n",
    "for num in [10 + k * 5 for k in range(23)]:\n",
    "    N = num\n",
    "    num_instances = 10000\n",
    "\n",
    "    file = \"SK_model_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "    print(file)    \n",
    "    data_file = h5py.File(PATH + file, \"r\")\n",
    "\n",
    "    E_star = 0\n",
    "    E_star_squared = 0\n",
    "    for i in range(num_instances):\n",
    "        J = data_file[\"set_\" + str(i + 1) + \"/J\"][:]\n",
    "        S_z = data_file[\"set_\" + str(i + 1) + \"/S_z\"][:]\n",
    "\n",
    "        E_star += expectation(solution(S_z), J)\n",
    "        E_star_squared += expectation(solution(S_z), J)**2\n",
    "        \n",
    "    out_file = h5py.File(PATH_DB + file, \"w\")\n",
    "    out_file.create_dataset(\"E_star\", data=[E_star / num_instances])\n",
    "    out_file.create_dataset(\"E_star_squared\", data=[E_star_squared / num_instances])\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace5767",
   "metadata": {},
   "source": [
    "## Number Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb11fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule\n",
    "p = 10000\n",
    "τ = 0.25\n",
    "\n",
    "γ = τ * (np.arange(1, p + 1) - 1/2) / p\n",
    "β = τ * (1 - np.arange(1, p + 1) / p)\n",
    "β[p-1] = τ / (4 * p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375043a",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"../data/number-partitioning/instances/batch_06/\"\n",
    "\n",
    "# X = np.linspace(4, 20, 9, dtype=int)\n",
    "# ENSEMBLE_SIZE = 100\n",
    "\n",
    "# for n in X:\n",
    "#     for i in range(ENSEMBLE_SIZE):\n",
    "        \n",
    "#         filename = \"number_partitioning_num_{0:02d}\".format(n) + \"_replica_{0:03d}\".format(i + 1)\n",
    "#         print(i, \", \", filename, \"\\n\")\n",
    "        \n",
    "#         a = np.load(PATH + filename + \".npy\")\n",
    "#         J = -2 * np.outer(a.T, a)\n",
    "#         np.fill_diagonal(J, 0.)    \n",
    "        \n",
    "#         # freeze final spin to +1\n",
    "#         h_z = J[-1, :-1]\n",
    "#         J = J[:-1, :-1]\n",
    "#         print(J, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ca4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/lappet/Archives/projects/QuantumComputing/QAA_and_MFQAOA/data/mean-field-qaoa/results/number-partitioning/batch_06/new/\"\n",
    "PATH_DB = \"/home/lappet/Dropbox/MidnightRambler/Mean_Field_AOA/Figures/Data/NP_problem/new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e1c216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:00<08:25, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_4_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:23<00:00, 26.05it/s]\n",
      "  0%|          | 3/10000 [00:00<06:40, 24.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_6_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:27<00:00, 25.81it/s]\n",
      "  0%|          | 3/10000 [00:00<06:45, 24.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_8_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:28<00:00, 25.75it/s]\n",
      "  0%|          | 3/10000 [00:00<06:04, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_10_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:27<00:00, 25.80it/s]\n",
      "  0%|          | 3/10000 [00:00<06:33, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_12_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:27<00:00, 25.78it/s]\n",
      "  0%|          | 3/10000 [00:00<06:56, 24.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_14_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:30<00:00, 25.58it/s]\n",
      "  0%|          | 3/10000 [00:00<07:41, 21.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_16_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:34<00:00, 25.35it/s]\n",
      "  0%|          | 3/10000 [00:00<06:37, 25.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_18_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:36<00:00, 25.19it/s]\n",
      "  0%|          | 3/10000 [00:00<07:05, 23.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_20_num_inst_10000_seed_137.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:32<00:00, 25.48it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(4, 20, 9, dtype=int)\n",
    "\n",
    "for N in X:\n",
    "    seed = 137\n",
    "    np.random.seed(seed)\n",
    "    num_instances = 10000\n",
    "\n",
    "    file = \"number-partitioning_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "    print(file)\n",
    "\n",
    "    h5_file = h5py.File(PATH + file, \"w\")\n",
    "\n",
    "    # Number partitioning\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(num_instances)):\n",
    "        a = np.random.uniform(0, 1, size=N) \n",
    "        a = np.sort(a)\n",
    "        J = -2 * np.outer(a.T, a)\n",
    "        np.fill_diagonal(J, 0.)    \n",
    "        \n",
    "        S = np.array([[1., 0., 0.] for _ in range(N - 1)]) # fix final spin (i.e. leave it out)\n",
    "        S = evolve(S, J, β, γ)\n",
    "\n",
    "        h5_file.create_dataset(\"set_\" + str(i + 1) + \"/a\", data=a)\n",
    "        h5_file.create_dataset(\"set_\" + str(i + 1) + \"/S_z\", data=S[:, 2])\n",
    "\n",
    "    h5_file.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a03a91",
   "metadata": {},
   "source": [
    "### Histogram statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "362c657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-partitioning_p_1000_N_10_num_inst_10000_seed_137.h5\n"
     ]
    }
   ],
   "source": [
    "seed = 137\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10\n",
    "num_instances = 10000\n",
    "\n",
    "file = \"number-partitioning_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "print(file)    \n",
    "data_file = h5py.File(PATH + file, \"r\")\n",
    "\n",
    "E_stars = []\n",
    "for i in range(num_instances):\n",
    "    a = data_file[\"set_\" + str(i + 1) + \"/a\"][:]\n",
    "    S_z = data_file[\"set_\" + str(i + 1) + \"/S_z\"][:]\n",
    "    \n",
    "    J = -2 * np.outer(a.T, a)\n",
    "    np.fill_diagonal(J, 0.)   \n",
    "    \n",
    "    import copy\n",
    "    E_and_string = (np.sum(a**2) + expectation(solution(S_z), J), S_z)\n",
    "    \n",
    "#     # one spin flip\n",
    "#     for j in range(S_z.shape[0]):\n",
    "#         new_S_z = copy.deepcopy(S_z)\n",
    "#         new_S_z[j] *= -1\n",
    "#         if E_and_string[0] > np.sum(a**2) + expectation(solution(new_S_z), J):\n",
    "#             E_and_string = (np.sum(a**2) + expectation(solution(new_S_z), J), new_S_z)\n",
    "\n",
    "    # two spin flips\n",
    "    for j in range(S_z.shape[0]):\n",
    "        for k in range(j + 1, S_z.shape[0]):\n",
    "            new_S_z = copy.deepcopy(S_z)\n",
    "            new_S_z[j] *= -1\n",
    "            new_S_z[k] *= -1\n",
    "            if E_and_string[0] > np.sum(a**2) + expectation(solution(new_S_z), J):\n",
    "                E_and_string = (np.sum(a**2) + expectation(solution(new_S_z), J), new_S_z)\n",
    "    \n",
    "    E_stars.append(np.sqrt(E_and_string[0]))\n",
    "\n",
    "file = \"number-partitioning_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \"_all_E_stars.h5\"    \n",
    "out_file = h5py.File(PATH_DB + file, \"w\")\n",
    "out_file.create_dataset(\"all_E_stars\", data=E_stars)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52faf826",
   "metadata": {},
   "source": [
    "### Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_0 = -0.763\n",
    "\n",
    "for N in X:\n",
    "    num_instances = 10000\n",
    "\n",
    "    file = \"number-partitioning_\" + \"p_\" + str(p) + \"_N_\" + str(N) + \"_num_inst_\" + str(num_instances) + \"_seed_\" + str(seed) + \".h5\"\n",
    "    print(file)    \n",
    "    data_file = h5py.File(PATH + file, \"r\")\n",
    "\n",
    "    E_star_sqrt = 0\n",
    "    E_star_squared = 0\n",
    "    for i in range(num_instances):\n",
    "        a = data_file[\"set_\" + str(i + 1) + \"/a\"][:]\n",
    "        S_z = data_file[\"set_\" + str(i + 1) + \"/S_z\"][:]\n",
    "\n",
    "        J = -2 * np.outer(a.T, a)\n",
    "        np.fill_diagonal(J, 0.)\n",
    "\n",
    "        E_star_sqrt += np.sqrt(np.sum(a**2) + expectation(solution(S_z), J))\n",
    "#         E_star_squared += expectation(solution(S_z), J)**2\n",
    "        \n",
    "    out_file = h5py.File(PATH_DB + file, \"w\")\n",
    "    out_file.create_dataset(\"E_star_sqrt\", data=[E_star_sqrt / num_instances])\n",
    "#     out_file.create_dataset(\"E_star_squared\", data=[E_star_squared / num_instances])\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d339609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
